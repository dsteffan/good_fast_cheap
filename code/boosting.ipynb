{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /Users/derek/anaconda3/lib/python3.7/site-packages (0.82)\r\n",
      "Requirement already satisfied: scipy in /Users/derek/anaconda3/lib/python3.7/site-packages (from xgboost) (1.2.1)\r\n",
      "Requirement already satisfied: numpy in /Users/derek/anaconda3/lib/python3.7/site-packages (from xgboost) (1.16.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, confusion_matrix, roc_auc_score \n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remy's \"print-out-every-metric-you-could-ever-possibly-need\" method\n",
    "\n",
    "def clf_metrics(model,X,y,model_name):\n",
    "    preds = model.predict(X)\n",
    "    accuracy = accuracy_score(y,preds)\n",
    "    roc_auc = roc_auc_score(y,preds)\n",
    "    c_matrix = confusion_matrix(y, preds)\n",
    "    tn, fp, fn, tp = c_matrix.ravel()\n",
    "    sensitivity = tp/(tp+fn)\n",
    "    specificity = tn/(tn+fp)\n",
    "    precision = tp/(tp+fp)\n",
    "    f1_score = 2*tp/(2*tp + fp + fn)\n",
    "    \n",
    "    metric_dict = {\n",
    "        'Confusion Matrix':c_matrix,\n",
    "        'Accuracy':accuracy,\n",
    "        'ROC AUC':roc_auc,\n",
    "        'Sensitivity':sensitivity,\n",
    "        'Specificity':specificity,\n",
    "        'Precision':precision,\n",
    "        'F1 Score':f1_score\n",
    "    }\n",
    "    c_matrix_df = pd.DataFrame([[tn,fn],[fp,tp]],\n",
    "                               columns = ['Actual Negative','Actual Positive'],\n",
    "                               index=['Predicted Negative','Predicted Positive'])\n",
    "    \n",
    "    # Print Block\n",
    "    print('-'*75)\n",
    "    print(f'Classification Metrics for {model_name}')\n",
    "    print('')\n",
    "    print(f'Accuracy:\\t\\t{round(accuracy,4)}')\n",
    "    print(f'Sensitivity:\\t\\t{round(sensitivity,4)}')\n",
    "    print(f'Specificity:\\t\\t{round(specificity,4)}')\n",
    "    print(f'Precision:\\t\\t{round(precision,4)}')\n",
    "    print(f'F1 Score:\\t\\t{round(f1_score,4)}')\n",
    "    print(f'ROC AUC:\\t\\t{round(roc_auc,4)}')\n",
    "    print(f'Confusion Matrix:\\n{c_matrix_df}')\n",
    "    print('-'*75)\n",
    "    # print('')\n",
    "    \n",
    "    return metric_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "cheap = pd.read_csv(\"../data/cheap_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = (cheap[\"wage\"] == 1).map({True: 3, False: 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age',\n",
       " 'education-num',\n",
       " 'sex',\n",
       " 'capital-gain',\n",
       " 'capital-loss',\n",
       " 'hours-per-week',\n",
       " 'is_investor',\n",
       " 'sum_investments',\n",
       " 'full_time',\n",
       " 'workclass_Federal-gov',\n",
       " 'workclass_Local-gov',\n",
       " 'workclass_Never-worked',\n",
       " 'workclass_Private',\n",
       " 'workclass_Self-emp-inc',\n",
       " 'workclass_Self-emp-not-inc',\n",
       " 'workclass_State-gov',\n",
       " 'workclass_Without-pay',\n",
       " 'occupation_Adm-clerical',\n",
       " 'occupation_Armed-Forces',\n",
       " 'occupation_Craft-repair',\n",
       " 'occupation_Exec-managerial',\n",
       " 'occupation_Farming-fishing',\n",
       " 'occupation_Handlers-cleaners',\n",
       " 'occupation_Machine-op-inspct',\n",
       " 'occupation_Other-service',\n",
       " 'occupation_Priv-house-serv',\n",
       " 'occupation_Prof-specialty',\n",
       " 'occupation_Protective-serv',\n",
       " 'occupation_Sales',\n",
       " 'occupation_Tech-support',\n",
       " 'occupation_Transport-moving',\n",
       " 'marital-status_Divorced',\n",
       " 'marital-status_Married-AF-spouse',\n",
       " 'marital-status_Married-civ-spouse',\n",
       " 'marital-status_Married-spouse-absent',\n",
       " 'marital-status_Never-married',\n",
       " 'marital-status_Separated',\n",
       " 'marital-status_Widowed',\n",
       " 'relationship_Husband',\n",
       " 'relationship_Not-in-family',\n",
       " 'relationship_Other-relative',\n",
       " 'relationship_Own-child',\n",
       " 'relationship_Unmarried',\n",
       " 'relationship_Wife']"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = [col for col in cheap.columns if col not in [\"fnlwgt\", \"wage\"] and cheap[col].dtype != \"object\"]\n",
    "\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cheap[features]\n",
    "y = cheap[\"wage\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    random_state = 35,\n",
    "                                                    stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "Classification Metrics for XGBoost\n",
      "\n",
      "Accuracy:\t\t0.8702\n",
      "Sensitivity:\t\t0.631\n",
      "Specificity:\t\t0.9461\n",
      "Precision:\t\t0.7877\n",
      "F1 Score:\t\t0.7007\n",
      "ROC AUC:\t\t0.7885\n",
      "Confusion Matrix:\n",
      "                    Actual Negative  Actual Positive\n",
      "Predicted Negative             3508              434\n",
      "Predicted Positive              200              742\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "xgbc = XGBClassifier()\n",
    "\n",
    "# Pulling out the weights based on \"wage\" column\n",
    "weights = (cheap[\"wage\"] == 1).map({True: 3, False: 1})\n",
    "\n",
    "# Using \"fnlwgt\" gives a bit better results\n",
    "metrics = clf_metrics(xgbc.fit(X_train, y_train, sample_weight = cheap[\"fnlwgt\"].to_list()), \n",
    "                      X_train, y_train, \"XGBoost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "Classification Metrics for XGBoost\n",
      "\n",
      "Accuracy:\t\t0.8699\n",
      "Sensitivity:\t\t0.6148\n",
      "Specificity:\t\t0.9507\n",
      "Precision:\t\t0.798\n",
      "F1 Score:\t\t0.6945\n",
      "ROC AUC:\t\t0.7827\n",
      "Confusion Matrix:\n",
      "                    Actual Negative  Actual Positive\n",
      "Predicted Negative             1176              151\n",
      "Predicted Positive               61              241\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "metrics = clf_metrics(xgbc.fit(X_train, y_train, sample_weight = cheap[\"fnlwgt\"].to_list()), \n",
    "                      X_test, y_test, \"XGBoost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/derek/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Users/derek/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  \n",
      "/Users/derek/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# Scaling\n",
    "ss = StandardScaler()\n",
    "\n",
    "ss.fit(X_train)\n",
    "\n",
    "X_train_sc = ss.transform(X_train)\n",
    "X_test_sc  = ss.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# Principle component analysis\n",
    "pca = PCA()\n",
    "\n",
    "pca.fit(X_train)\n",
    "\n",
    "X_train_pca = pca.transform(X_train_sc)\n",
    "X_test_pca  = pca.transform(X_test_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99732159, 0.99999655, 0.99999845, 0.99999991, 0.99999997,\n",
       "       0.99999998, 0.99999998, 0.99999998, 0.99999998, 0.99999999,\n",
       "       0.99999999, 0.99999999, 0.99999999, 0.99999999, 0.99999999,\n",
       "       0.99999999, 0.99999999, 0.99999999, 0.99999999, 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        ])"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_ratio_.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca_8 = pd.DataFrame(X_train_pca)[range(8)]\n",
    "X_test_pca_8  = pd.DataFrame(X_test_pca)[range(8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "Classification Metrics for XGBoost\n",
      "\n",
      "Accuracy:\t\t0.8735\n",
      "Sensitivity:\t\t0.648\n",
      "Specificity:\t\t0.945\n",
      "Precision:\t\t0.7888\n",
      "F1 Score:\t\t0.7115\n",
      "ROC AUC:\t\t0.7965\n",
      "Confusion Matrix:\n",
      "                    Actual Negative  Actual Positive\n",
      "Predicted Negative             3504              414\n",
      "Predicted Positive              204              762\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "metrics = clf_metrics(xgbc.fit(X_train_pca_8, y_train, sample_weight = cheap[\"fnlwgt\"].to_list()), \n",
    "                      X_train_pca_8, y_train, \"XGBoost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "Classification Metrics for XGBoost\n",
      "\n",
      "Accuracy:\t\t0.8735\n",
      "Sensitivity:\t\t0.6224\n",
      "Specificity:\t\t0.9531\n",
      "Precision:\t\t0.8079\n",
      "F1 Score:\t\t0.7032\n",
      "ROC AUC:\t\t0.7878\n",
      "Confusion Matrix:\n",
      "                    Actual Negative  Actual Positive\n",
      "Predicted Negative             1179              148\n",
      "Predicted Positive               58              244\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "metrics = clf_metrics(xgbc.fit(X_train_pca, y_train, sample_weight = cheap[\"fnlwgt\"].to_list()), \n",
    "                      X_test_pca, y_test, \"XGBoost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/derek/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Users/derek/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "/Users/derek/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:18: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "Classification Metrics for XGBoost\n",
      "\n",
      "Accuracy:\t\t0.8655\n",
      "Sensitivity:\t\t0.6241\n",
      "Specificity:\t\t0.942\n",
      "Precision:\t\t0.7734\n",
      "F1 Score:\t\t0.6908\n",
      "ROC AUC:\t\t0.7831\n",
      "Confusion Matrix:\n",
      "                    Actual Negative  Actual Positive\n",
      "Predicted Negative             3493              442\n",
      "Predicted Positive              215              734\n",
      "---------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------\n",
      "Classification Metrics for XGBoost\n",
      "\n",
      "Accuracy:\t\t0.8527\n",
      "Sensitivity:\t\t0.5918\n",
      "Specificity:\t\t0.9353\n",
      "Precision:\t\t0.7436\n",
      "F1 Score:\t\t0.6591\n",
      "ROC AUC:\t\t0.7636\n",
      "Confusion Matrix:\n",
      "                    Actual Negative  Actual Positive\n",
      "Predicted Negative             1157              160\n",
      "Predicted Positive               80              232\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Final XGBoost model\n",
    "\n",
    "features = [col for col in cheap.columns if col not in [\"fnlwgt\", \"wage\"] and cheap[col].dtype != \"object\"]\n",
    "\n",
    "X = cheap[features]\n",
    "y = cheap[\"wage\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    random_state = 35,\n",
    "                                                    stratify = y)\n",
    "\n",
    "# Scaling\n",
    "ss = StandardScaler()\n",
    "\n",
    "ss.fit(X_train)\n",
    "\n",
    "X_train_sc = ss.transform(X_train)\n",
    "X_test_sc  = ss.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# Principle component analysis\n",
    "pca = PCA()\n",
    "\n",
    "pca.fit(X_train_sc)\n",
    "\n",
    "X_train_pca = pca.transform(X_train_sc)\n",
    "X_test_pca  = pca.transform(X_test_sc)\n",
    "\n",
    "X_train_pca_8 = pd.DataFrame(X_train_pca)[range(8)]\n",
    "X_test_pca_8  = pd.DataFrame(X_test_pca)[range(8)]\n",
    "\n",
    "metrics = clf_metrics(xgbc.fit(X_train_pca_8, y_train, sample_weight = cheap[\"fnlwgt\"].to_list()), \n",
    "                      X_train_pca_8, y_train, \"XGBoost\")\n",
    "\n",
    "metrics = clf_metrics(xgbc.fit(X_train_pca, y_train, sample_weight = cheap[\"fnlwgt\"].to_list()), \n",
    "                      X_test_pca, y_test, \"XGBoost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"../data/test_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/derek/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Users/derek/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:18: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "/Users/derek/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "/Users/derek/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:20: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "Classification Metrics for XGBoost\n",
      "\n",
      "Accuracy:\t\t0.8655\n",
      "Sensitivity:\t\t0.6241\n",
      "Specificity:\t\t0.942\n",
      "Precision:\t\t0.7734\n",
      "F1 Score:\t\t0.6908\n",
      "ROC AUC:\t\t0.7831\n",
      "Confusion Matrix:\n",
      "                    Actual Negative  Actual Positive\n",
      "Predicted Negative             3493              442\n",
      "Predicted Positive              215              734\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Final XGBoost model\n",
    "\n",
    "features = [col for col in cheap.columns if col not in [\"fnlwgt\", \"wage\"] and cheap[col].dtype != \"object\"]\n",
    "\n",
    "X = cheap[features]\n",
    "y = cheap[\"wage\"]\n",
    "X_prog = test[features]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    random_state = 35,\n",
    "                                                    stratify = y)\n",
    "\n",
    "# Scaling\n",
    "ss = StandardScaler()\n",
    "\n",
    "ss.fit(X_train)\n",
    "\n",
    "X_train_sc = ss.transform(X_train)\n",
    "X_test_sc  = ss.transform(X_test)\n",
    "X_prog_sc  = ss.transform(X_prog)\n",
    "\n",
    "\n",
    "\n",
    "# Principle component analysis\n",
    "pca = PCA()\n",
    "\n",
    "pca.fit(X_train_sc)\n",
    "\n",
    "X_train_pca = pca.transform(X_train_sc)\n",
    "X_test_pca  = pca.transform(X_test_sc)\n",
    "X_prog_pca  = pca.transform(X_prog_sc)\n",
    "\n",
    "X_train_pca_8 = pd.DataFrame(X_train_pca)[range(8)]\n",
    "X_test_pca_8  = pd.DataFrame(X_test_pca)[range(8)]\n",
    "X_prog_pca_8  = pd.DataFrame(X_prog_pca)[range(8)]\n",
    "\n",
    "xgbc = XGBClassifier()\n",
    "xgbc.fit(X_train_pca_8, y_train, sample_weight = cheap[\"fnlwgt\"].to_list())\n",
    "\n",
    "metrics = clf_metrics(xgbc.fit(X_train_pca_8, y_train, sample_weight = cheap[\"fnlwgt\"].to_list()), \n",
    "                      X_train_pca_8, y_train, \"XGBoost\")\n",
    "\n",
    "predictions = xgbc.predict(X_prog_pca_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(predictions).to_csv(\"../data/predictions.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking methods\n",
    "# Our group decided to just use PCA XGBoost for simplicity.\n",
    "\n",
    "# #### Predictions for LogReg\n",
    "# lr = LogisticRegression()\n",
    "# lr.fit(X_train_sc, y_train)\n",
    "# test['logreg_pred'] = lr.predict(X_prog_sc)\n",
    "\n",
    "# #### Predictions for Decision Tree\n",
    "# dt = DecisionTreeClassifier()\n",
    "# dt.fit(X_train_sc, y_train)\n",
    "# test['dec_tree_pred'] = dt.predict(X_prog_sc)\n",
    "\n",
    "# #### Predictions for XGBoost\n",
    "# xgbc = XGBClassifier()\n",
    "# xgbc.fit(X_train_pca, y_train, sample_weight = cheap[\"fnlwgt\"].to_list())\n",
    "# test['xgbc_pca_pred'] = xgbc.predict(X_prog_pca_8)\n",
    "\n",
    "\n",
    "\n",
    "# ### 1) Have 3 models make class predictions and append them to DataFrame\n",
    "# ### 2) Function takes DataFrame and 3 predictions and outputs a aggregate prediction\n",
    "# def stack_models(df, pred1, pred2, pred3):\n",
    "#     df['combo'] = pred1 + pred2 + pred3\n",
    "\n",
    "#     for i, row in enumerate(df.iterrows()):\n",
    "#         if row[1]['combo'] >= 2:\n",
    "#             df.loc[row[0],'combo_pred'] = 1\n",
    "#         else:\n",
    "#             df.loc[row[0],'combo_pred'] = 0\n",
    "#     return df\n",
    "\n",
    "\n",
    "# stack_models(test, test['xgbc_pca_pred'], test['logreg_pred'], test['dec_tree_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
